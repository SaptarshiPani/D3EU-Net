{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10316661,"sourceType":"datasetVersion","datasetId":6118595}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.distributions as dist\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nimport torchvision\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nfrom PIL import Image\nimport re\nfrom collections import Counter\n\n# Text Encoder with Embedding and Transformer\nclass TextEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_layers=2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim, \n            nhead=num_heads,\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.final_fc = nn.Linear(embed_dim, embed_dim)\n        \n    def forward(self, text_tokens):\n        # text_tokens: [batch_size, seq_len]\n        embedded = self.embedding(text_tokens)  # [batch_size, seq_len, embed_dim]\n        encoded = self.transformer(embedded)  # [batch_size, seq_len, embed_dim]\n        # Use mean pooling for global representation\n        pooled = torch.mean(encoded, dim=1)  # [batch_size, embed_dim]\n        return self.final_fc(pooled)\n\n# Spatial Encoder for masks\nclass SpatialEncoder(nn.Module):\n    def __init__(self, in_channels=1, base_channels=64):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n        self.conv2 = nn.Conv2d(base_channels, base_channels*2, 3, padding=1, stride=2)\n        self.conv3 = nn.Conv2d(base_channels*2, base_channels*4, 3, padding=1, stride=2)\n        self.conv4 = nn.Conv2d(base_channels*4, base_channels*8, 3, padding=1, stride=2)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n        \n    def forward(self, x):\n        x1 = F.relu(self.conv1(x))    # [B, 64, H, W]\n        x2 = F.relu(self.conv2(x1))   # [B, 128, H/2, W/2]\n        x3 = F.relu(self.conv3(x2))   # [B, 256, H/4, W/4]\n        x4 = F.relu(self.conv4(x3))   # [B, 512, H/8, W/8]\n        spatial_feat = self.adaptive_pool(x4)  # [B, 512, 1, 1]\n        return spatial_feat.squeeze(-1).squeeze(-1), [x1, x2, x3, x4]\n\n# Cross-Attention Module\nclass CrossAttention(nn.Module):\n    def __init__(self, channels, text_dim, num_heads=4):\n        super().__init__()\n        self.norm = nn.LayerNorm(channels)\n        self.text_proj = nn.Linear(text_dim, channels)\n        self.attn = nn.MultiheadAttention(channels, num_heads, batch_first=True)\n        \n    def forward(self, x, text_embed):\n        # x: [B, C, H, W]\n        # text_embed: [B, text_dim]\n        B, C, H, W = x.shape\n        x_flat = x.permute(0, 2, 3, 1).reshape(B, H*W, C)  # [B, H*W, C]\n        x_flat = self.norm(x_flat)\n        \n        # Project text to same dimension\n        text_proj = self.text_proj(text_embed).unsqueeze(1)  # [B, 1, C]\n        \n        # Cross-attention\n        attn_out, _ = self.attn(\n            query=x_flat, \n            key=text_proj,\n            value=text_proj\n        )\n        \n        # Residual connection\n        attn_out = attn_out.reshape(B, H, W, C).permute(0, 3, 1, 2)\n        return x + attn_out\n\n# Modified UNet with attention and conditioning\nclass UNet(nn.Module):\n    def __init__(self, input_channels=3, output_channels=1, text_dim=128, spatial_dim=512):\n        super(UNet, self).__init__()\n        \n        # Encoder\n        self.enc1 = self.conv_block(input_channels, 64)\n        self.enc2 = self.conv_block(64, 128)\n        self.enc3 = self.conv_block(128, 256)\n        self.enc4 = self.conv_block(256, 512)\n        \n        # Bottleneck\n        self.bottleneck = self.conv_block(512, 1024)\n        \n        # Decoder with attention\n        self.up4 = self.upconv(1024, 512)\n        self.dec4 = self.conv_block(1024, 512)\n        self.attn4 = CrossAttention(512, text_dim + spatial_dim)\n        \n        self.up3 = self.upconv(512, 256)\n        self.dec3 = self.conv_block(512, 256)\n        self.attn3 = CrossAttention(256, text_dim + spatial_dim)\n        \n        self.up2 = self.upconv(256, 128)\n        self.dec2 = self.conv_block(256, 128)\n        self.attn2 = CrossAttention(128, text_dim + spatial_dim)\n        \n        self.up1 = self.upconv(128, 64)\n        self.dec1 = self.conv_block(128, 64)\n        self.attn1 = CrossAttention(64, text_dim + spatial_dim)\n        \n        # Final layers\n        self.final = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n        self.final2 = nn.Conv2d(3, output_channels, kernel_size=1)\n        self.out_act = nn.Sigmoid()\n        \n    def conv_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n    \n    def upconv(self, in_channels, out_channels):\n        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n    \n    def forward(self, x, mask, text_embed, spatial_embed):\n        # Combine text and spatial features\n        cond_embed = torch.cat([text_embed, spatial_embed], dim=1)\n        \n        # Encoder\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(F.max_pool2d(enc1, 2))\n        enc3 = self.enc3(F.max_pool2d(enc2, 2))\n        enc4 = self.enc4(F.max_pool2d(enc3, 2))\n        \n        # Bottleneck\n        bottleneck = self.bottleneck(F.max_pool2d(enc4, 2))\n        \n        # Decoder with attention\n        up4 = self.up4(bottleneck)\n        dec4_in = torch.cat((up4, enc4), dim=1)\n        dec4 = self.dec4(dec4_in)\n        dec4 = self.attn4(dec4, cond_embed)\n        \n        up3 = self.up3(dec4)\n        dec3_in = torch.cat((up3, enc3), dim=1)\n        dec3 = self.dec3(dec3_in)\n        dec3 = self.attn3(dec3, cond_embed)\n        \n        up2 = self.up2(dec3)\n        dec2_in = torch.cat((up2, enc2), dim=1)\n        dec2 = self.dec2(dec2_in)\n        dec2 = self.attn2(dec2, cond_embed)\n        \n        up1 = self.up1(dec2)\n        dec1_in = torch.cat((up1, enc1), dim=1)\n        dec1 = self.dec1(dec1_in)\n        dec1 = self.attn1(dec1, cond_embed)\n        \n        # Output\n        out = self.final(dec1)\n        return self.out_act(self.final2(x - out))\n\nclass Diffusion:\n    def __init__(self, T=1000, beta_start=1e-4, beta_end=0.02, device=False):\n        self.device = device\n        self.T = T\n        self.betas = torch.linspace(beta_start, beta_end, T)\n        self.alphas = 1.0 - self.betas\n        self.alpha_hat = torch.cumprod(self.alphas, dim=0).to(device)\n    \n    def q_sample(self, x_start, t, noise=None):\n        if noise is None:\n            noise = torch.randn_like(x_start)\n        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n        return sqrt_alpha_hat * x_start + sqrt_one_minus_alpha_hat * noise, noise\n\n# Vocabulary and Tokenizer\nclass Vocabulary:\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.idx = 0\n        self.add_word('<pad>')\n        self.add_word('<unk>')\n        \n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n            \n    def build_from_texts(self, texts, min_freq=1):\n        counter = Counter()\n        for text in texts:\n            words = self.tokenize(text)\n            counter.update(words)\n        \n        words = [word for word, count in counter.items() if count >= min_freq]\n        for word in words:\n            self.add_word(word)\n    \n    def tokenize(self, text):\n        text = text.lower()\n        text = re.sub(r'[^\\w\\s]', '', text)\n        return text.split()\n    \n    def encode(self, text, max_len=20):\n        words = self.tokenize(text)\n        tokens = [self.word2idx.get(word, self.word2idx['<unk>']) for word in words]\n        \n        # Pad or truncate\n        if len(tokens) > max_len:\n            tokens = tokens[:max_len]\n        else:\n            tokens += [self.word2idx['<pad>']] * (max_len - len(tokens))\n        return tokens\n\n# Modified Dataset Class\nclass SegmentationDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, text_prompts=None, \n                 transform=None, target_transform=None, vocab=None, max_len=20):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.image_filenames = sorted(os.listdir(image_dir))\n        self.mask_filenames = sorted(os.listdir(mask_dir))\n        self.transform = transform\n        self.target_transform = target_transform\n        self.max_len = max_len\n        \n        # Text prompts\n        if text_prompts is None:\n            # Generate default prompts if not provided\n            self.text_prompts = [\n                \"{Prompt}\"\n                for _ in range(len(self.image_filenames))\n            ]\n        else:\n            self.text_prompts = text_prompts\n            \n        # Build vocabulary if needed\n        self.vocab = vocab\n        if self.vocab is None:\n            self.vocab = Vocabulary()\n            self.vocab.build_from_texts(self.text_prompts)\n    \n    def __len__(self):\n        return len(self.image_filenames)\n\n    def __getitem__(self, idx):\n        # Load image and mask\n        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n        \n        image = Image.open(image_path).convert(\"RGB\")\n        mask = Image.open(mask_path).convert(\"L\")\n        \n        # Apply transformations\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            mask = self.target_transform(mask)\n        \n        # Process text\n        text = self.text_prompts[idx]\n        text_tokens = self.vocab.encode(text, self.max_len)\n        \n        return image, mask, torch.tensor(text_tokens, dtype=torch.long)\n\n# Define transformations\nimage_transform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\nmask_transform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n])\n\n# Paths to dataset\ntrain_image_dir = \"\"\ntrain_mask_dir = \"\"\ntest_image_dir = \"\"\ntest_mask_dir = \"\"\n\n# Create datasets\ntrain_dataset = SegmentationDataset(\n    train_image_dir, train_mask_dir, \n    transform=image_transform, \n    target_transform=mask_transform\n)\n\ntest_dataset = SegmentationDataset(\n    test_image_dir, test_mask_dir, \n    transform=image_transform, \n    target_transform=mask_transform,\n    vocab=train_dataset.vocab  # Share vocabulary\n)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n\n# Check a sample batch\nif __name__ == \"__main__\":\n    for images, masks, texts in train_loader:\n        print(\"Image batch shape:\", images.shape)\n        print(\"Mask batch shape:\", masks.shape)\n        print(\"Text tokens shape:\", texts.shape)\n        print(\"Sample text tokens:\", texts[0])\n        break\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize models\nmodel = UNet(\n    input_channels=3, \n    output_channels=1,\n    text_dim=128,\n    spatial_dim=512\n).to(device)\n\ntext_encoder = TextEncoder(\n    vocab_size=len(train_dataset.vocab.word2idx),\n    embed_dim=128\n).to(device)\n\nspatial_encoder = SpatialEncoder(\n    in_channels=1,\n    base_channels=64\n).to(device)\n\ndiffusion = Diffusion(T=1, device=device)\n\n# Loss and optimizer\ncriterion = nn.BCELoss()\nparams = list(model.parameters()) + list(text_encoder.parameters()) + list(spatial_encoder.parameters())\noptimizer = torch.optim.Adam(params, lr=1e-3)\n\n# Training loop\nfor epoch in range(1000):\n    model.train()\n    text_encoder.train()\n    spatial_encoder.train()\n    \n    epoch_loss = 0\n    for images, masks, texts in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        images, masks, texts = images.to(device), masks.to(device), texts.to(device)\n        \n        # Encode text and spatial features\n        text_embed = text_encoder(texts)\n        spatial_embed, _ = spatial_encoder(masks)\n        \n        # Sample timestep\n        t = torch.randint(0, diffusion.T, (images.size(0),)).to(device)\n        \n        # Add noise to images\n        noisy_images, noise = diffusion.q_sample(images, t)\n        \n        # Predict noise with conditioning\n        noise_pred = model(noisy_images, masks, text_embed, spatial_embed)\n        \n        # Compute loss\n        loss = criterion(noise_pred, masks)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n\n    # Print loss\n    print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader):.4f}\")\n    \n    # Visualization\n    with torch.no_grad():\n        if epoch % 50 == 0:\n            text_encoder.eval()\n            spatial_encoder.eval()\n            \n            # Get sample batch\n            sample_images, sample_masks, sample_texts = images.cpu(), masks.cpu(), texts.cpu()\n            text_embed = text_encoder(texts.to(device))\n            spatial_embed, _ = spatial_encoder(masks.to(device))\n            noisy_sample_images = noisy_images.cpu()\n            generated_images = noise_pred.cpu()\n            \n            # Plot\n            fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n            axes[0].imshow(sample_images[0].permute(1, 2, 0))\n            axes[0].set_title(\"Input Image\")\n            axes[1].imshow(noisy_sample_images[0].permute(1, 2, 0))\n            axes[1].set_title(\"Noisy Image\")\n            axes[2].imshow(generated_images[0, 0], cmap='gray')\n            axes[2].set_title(\"Generated Mask\")\n            axes[3].imshow(sample_masks[0, 0], cmap='gray')\n            axes[3].set_title(\"Ground Truth Mask\")\n            \n            # Show text prompt\n            text_prompt = ' '.join([train_dataset.vocab.idx2word[idx.item()] for idx in sample_texts[0]])\n            axes[4].text(0.5, 0.5, text_prompt, fontsize=10, ha='center', va='center')\n            axes[4].axis('off')\n            axes[4].set_title(\"Text Prompt\")\n            \n            # Hide axes\n            for ax in axes[:4]:\n                ax.axis('off')\n            \n            plt.tight_layout()\n            plt.show()\n\n# Metrics calculation\ndef calculate_metrics(pred, target):\n    pred = (pred > 0.5).float()\n    target = target.float()\n    \n    intersection = torch.sum(pred * target)\n    union = torch.sum(pred) + torch.sum(target) - intersection\n    dice = (2.0 * intersection) / (torch.sum(pred) + torch.sum(target) + 1e-8)\n    \n    # Accuracy, Precision, Recall\n    true_positive = torch.sum(pred * target)\n    false_positive = torch.sum(pred * (1 - target))\n    false_negative = torch.sum((1 - pred) * target)\n    \n    accuracy = torch.sum(pred == target) / torch.numel(target)\n    precision = true_positive / (true_positive + false_positive + 1e-8)\n    recall = true_positive / (true_positive + false_negative + 1e-8)\n    \n    iou = intersection / (union + 1e-8)\n    \n    return iou.item(), dice.item(), accuracy.item(), precision.item(), recall.item()\n    \n# Evaluation function\ndef evaluate(model, text_encoder, spatial_encoder, dataloader, criterion, \n             diffusion, device, vocab, visualize=False):\n    model.eval()\n    text_encoder.eval()\n    spatial_encoder.eval()\n    \n    total_loss = 0\n    total_iou, total_dice = 0, 0\n    total_accuracy, total_precision, total_recall = 0, 0, 0\n    num_batches = 0\n    \n    with torch.no_grad():\n        for images, masks, texts in tqdm(dataloader, desc=\"Evaluating\"):\n            images, masks, texts = images.to(device), masks.to(device), texts.to(device)\n            \n            # Encode text and spatial features\n            text_embed = text_encoder(texts)\n            spatial_embed, _ = spatial_encoder(masks)\n            \n            # Sample timestep\n            t = torch.randint(0, diffusion.T, (images.size(0),)).to(device)\n\n            # Add noise to images\n            noisy_images, noise = diffusion.q_sample(images, t)\n\n            # Predict noise with conditioning\n            noise_pred = model(noisy_images, masks, text_embed, spatial_embed)\n            noise_pred = torch.sigmoid(noise_pred)\n\n            # Compute loss\n            loss = criterion(noise_pred, masks)\n            total_loss += loss.item()\n\n            # Calculate metrics\n            iou, dice, accuracy, precision, recall = calculate_metrics(noise_pred, masks)\n            total_iou += iou\n            total_dice += dice\n            total_accuracy += accuracy\n            total_precision += precision\n            total_recall += recall\n            num_batches += 1\n\n            # Visualization\n            if visualize and num_batches == 1:\n                sample_images = images.cpu()\n                noisy_sample_images = noisy_images.cpu()\n                generated_images = noise_pred.cpu()\n                sample_masks = masks.cpu()\n\n                # Plot examples\n                fig, axes = plt.subplots(4, 5, figsize=(20, 16))\n                for i in range(4):\n                    # Images\n                    axes[i, 0].imshow(sample_images[i].permute(1, 2, 0))\n                    axes[i, 0].set_title(\"Input Image\")\n                    axes[i, 1].imshow(noisy_sample_images[i].permute(1, 2, 0))\n                    axes[i, 1].set_title(\"Noisy Image\")\n                    axes[i, 2].imshow(generated_images[i, 0], cmap='gray')\n                    axes[i, 2].set_title(\"Predicted Mask\")\n                    axes[i, 3].imshow(sample_masks[i, 0], cmap='gray')\n                    axes[i, 3].set_title(\"Ground Truth Mask\")\n                    \n                    # Text prompt\n                    text_prompt = ' '.join([vocab.idx2word[idx.item()] for idx in texts[i]])\n                    axes[i, 4].text(0.5, 0.5, text_prompt, fontsize=10, ha='center', va='center')\n                    axes[i, 4].axis('off')\n                    axes[i, 4].set_title(\"Text Prompt\")\n                    \n                    # Hide axes\n                    for j in range(4):\n                        axes[i, j].axis('off')\n\n                plt.tight_layout()\n                plt.show()\n\n    # Compute average metrics\n    average_loss = total_loss / num_batches\n    average_iou = total_iou / num_batches\n    average_dice = total_dice / num_batches\n    average_accuracy = total_accuracy / num_batches\n    average_precision = total_precision / num_batches\n    average_recall = total_recall / num_batches\n\n    print(f\"Average Evaluation Loss: {average_loss:.4f}\")\n    print(f\"Average IoU: {average_iou:.4f}\")\n    print(f\"Average Dice Coefficient: {average_dice:.4f}\")\n    print(f\"Average Accuracy: {average_accuracy:.4f}\")\n    print(f\"Average Precision: {average_precision:.4f}\")\n    print(f\"Average Recall: {average_recall:.4f}\")\n    \n    return {\n        \"loss\": average_loss,\n        \"iou\": average_iou,\n        \"dice\": average_dice,\n        \"accuracy\": average_accuracy,\n        \"precision\": average_precision,\n        \"recall\": average_recall\n    }\n\n# Perform evaluation\nmetrics = evaluate(\n    model, text_encoder, spatial_encoder, \n    test_loader, criterion, diffusion, device,\n    vocab=train_dataset.vocab,\n    visualize=True\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}